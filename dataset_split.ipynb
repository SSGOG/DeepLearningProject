{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88721426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import yaml\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f47b431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images found: 16225\n",
      "Dataset split:\n",
      "Train: 10000 images\n",
      "Test: 3000 images\n",
      "Val: 3000 images\n",
      "Copying training images...\n",
      "Copying test images...\n",
      "Copying validation images...\n",
      "\n",
      "=== Final Dataset Statistics ===\n",
      "TRAIN:\n",
      "  Images: 10000\n",
      "  Labels: 10000\n",
      "  MILCO objects: 4919\n",
      "  NOMBO objects: 2581\n",
      "  Total objects: 7500\n",
      "VAL:\n",
      "  Images: 3000\n",
      "  Labels: 3000\n",
      "  MILCO objects: 1420\n",
      "  NOMBO objects: 753\n",
      "  Total objects: 2173\n",
      "TEST:\n",
      "  Images: 3000\n",
      "  Labels: 3000\n",
      "  MILCO objects: 1511\n",
      "  NOMBO objects: 833\n",
      "  Total objects: 2344\n",
      "\n",
      "✅ Dataset prepared! YAML file: sonar_dataset_10k_3k_3k.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('E:/sonardataset_10k_3k_3k')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_yolo_dataset_10k_3k_3k():\n",
    "    \"\"\"\n",
    "    Split dataset: 10,000 train, 3,000 test, 3,000 validation\n",
    "    Total: 16,000 images (leaving out 225 if you have 16,225)\n",
    "    \"\"\"\n",
    "    # Paths\n",
    "    augmented_path = Path(r\"D:\\VScodefiles\\DeepLearningProject\\Augmented\")\n",
    "    split_output = Path(r\"E:\\sonardataset_10k_3k_3k\")\n",
    "    \n",
    "    # Get all images\n",
    "    all_images = list((augmented_path / \"images\").glob(\"*.jpg\"))\n",
    "    print(f\"Total images found: {len(all_images)}\")\n",
    "    \n",
    "    # Shuffle randomly\n",
    "    random.shuffle(all_images)\n",
    "    \n",
    "    # Calculate splits\n",
    "    train_count = 10000\n",
    "    test_count = 3000\n",
    "    val_count = 3000\n",
    "    \n",
    "    # Verify we have enough images\n",
    "    total_needed = train_count + test_count + val_count\n",
    "    if len(all_images) < total_needed:\n",
    "        print(f\"❌ Not enough images! Have {len(all_images)}, need {total_needed}\")\n",
    "        return\n",
    "    \n",
    "    # Split the data\n",
    "    train_images = all_images[:train_count]\n",
    "    test_images = all_images[train_count:train_count + test_count]\n",
    "    val_images = all_images[train_count + test_count:train_count + test_count + val_count]\n",
    "    \n",
    "    print(f\"Dataset split:\")\n",
    "    print(f\"Train: {len(train_images)} images\")\n",
    "    print(f\"Test: {len(test_images)} images\")\n",
    "    print(f\"Val: {len(val_images)} images\")\n",
    "    \n",
    "    # Remove existing output directory\n",
    "    if split_output.exists():\n",
    "        shutil.rmtree(split_output)\n",
    "    \n",
    "    # Create directory structure\n",
    "    for split in ['train', 'test', 'val']:\n",
    "        (split_output / split / 'images').mkdir(parents=True, exist_ok=True)\n",
    "        (split_output / split / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Copy files to respective directories\n",
    "    def copy_split_files(image_list, split_name):\n",
    "        for img_path in image_list:\n",
    "            # Copy image\n",
    "            dst_img = split_output / split_name / 'images' / img_path.name\n",
    "            shutil.copy2(img_path, dst_img)\n",
    "            \n",
    "            # Copy corresponding label\n",
    "            label_path = augmented_path / 'labels' / img_path.with_suffix('.txt').name\n",
    "            dst_label = split_output / split_name / 'labels' / label_path.name\n",
    "            if label_path.exists():\n",
    "                shutil.copy2(label_path, dst_label)\n",
    "            else:\n",
    "                # Create empty label file\n",
    "                dst_label.touch()\n",
    "    \n",
    "    print(\"Copying training images...\")\n",
    "    copy_split_files(train_images, 'train')\n",
    "    \n",
    "    print(\"Copying test images...\")\n",
    "    copy_split_files(test_images, 'test')\n",
    "    \n",
    "    print(\"Copying validation images...\")\n",
    "    copy_split_files(val_images, 'val')\n",
    "    \n",
    "    # Create YAML file\n",
    "    yaml_content = {\n",
    "        'path': str(split_output.absolute()),\n",
    "        'train': 'train/images',\n",
    "        'val': 'val/images',\n",
    "        'test': 'test/images',\n",
    "        'nc': 2,\n",
    "        'names': ['MILCO', 'NOMBO']\n",
    "    }\n",
    "    \n",
    "    yaml_path = Path(\"sonar_dataset_10k_3k_3k.yaml\")\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(yaml_content, f, default_flow_style=False, sort_keys=False)\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(\"\\n=== Final Dataset Statistics ===\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        images_dir = split_output / split / 'images'\n",
    "        labels_dir = split_output / split / 'labels'\n",
    "        num_images = len(list(images_dir.glob('*.jpg')))\n",
    "        num_labels = len(list(labels_dir.glob('*.txt')))\n",
    "        \n",
    "        # Count objects per class - FIXED: Handle float values\n",
    "        class_counts = {0: 0, 1: 0}\n",
    "        for label_file in labels_dir.glob('*.txt'):\n",
    "            with open(label_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 1:\n",
    "                            try:\n",
    "                                # Handle both integer and float class IDs\n",
    "                                class_id = int(float(parts[0]))  # Convert float to int\n",
    "                                if class_id in class_counts:\n",
    "                                    class_counts[class_id] += 1\n",
    "                            except (ValueError, IndexError) as e:\n",
    "                                print(f\"Warning: Could not parse line in {label_file}: {line.strip()} - Error: {e}\")\n",
    "                                continue\n",
    "        \n",
    "        print(f\"{split.upper()}:\")\n",
    "        print(f\"  Images: {num_images}\")\n",
    "        print(f\"  Labels: {num_labels}\")\n",
    "        print(f\"  MILCO objects: {class_counts[0]}\")\n",
    "        print(f\"  NOMBO objects: {class_counts[1]}\")\n",
    "        print(f\"  Total objects: {class_counts[0] + class_counts[1]}\")\n",
    "    \n",
    "    print(f\"\\n✅ Dataset prepared! YAML file: {yaml_path}\")\n",
    "    return split_output\n",
    "\n",
    "# Run the preparation\n",
    "prepare_yolo_dataset_10k_3k_3k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438105df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
